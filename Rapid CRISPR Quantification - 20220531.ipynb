{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb5f1e6",
   "metadata": {},
   "source": [
    "# Rapid CRISPR Quantification - 20220531\n",
    "The following sections will guide you step by step to analyze CRISPR editted data using our software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048ae8d",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a32eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiffcapture in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: numpy>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiffcapture) (1.21.3)\n",
      "Requirement already satisfied: Pillow>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiffcapture) (8.2.0)\n",
      "Requirement already satisfied: tiffcapture in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: Pillow>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiffcapture) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiffcapture) (1.21.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiffcapture\n",
    "\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pickle\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy import ndimage\n",
    "import tiffcapture as tc\n",
    "import torch\n",
    "import tifffile\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def infer_mask(img):\n",
    "    mask = np.zeros_like(img)\n",
    "    percentile = np.percentile(img, 90)\n",
    "    indices = np.where(img > percentile)\n",
    "    mask[indices] = 1\n",
    "    return mask\n",
    "\n",
    "def draw_PSF(PSF_size, amp, sigma, loc):\n",
    "    x = np.arange(PSF_size)\n",
    "    y = np.arange(PSF_size)\n",
    "\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    PSF = (amp / (2 * np.pi * sigma ** 2) * np.exp(-((x+(loc[0]%1)-(PSF_size-1)/2) ** 2 / (2 * sigma ** 2)\n",
    "                                                        + (y+(loc[1]%1)-(PSF_size-1)/2) ** 2 / (2 * sigma ** 2))))\n",
    "    return PSF\n",
    "\n",
    "def AugmentData(data, damaged_site_range, size_increase):\n",
    "    [N, row, col] = data.shape\n",
    "    PSF_size = 9\n",
    "    # Arrays initialization\n",
    "    augmented_data = np.zeros([N * size_increase, row, col])\n",
    "    ground_truth = np.zeros([N * size_increase, row, col])\n",
    "    GT_counts = np.zeros([N * size_increase, 1])\n",
    "\n",
    "    # Building data base and saving ground truth\n",
    "    for i in range(N * size_increase):\n",
    "        # Infer the mask of the cell\n",
    "        augmented_data[i, :, :] = data[int(i / size_increase), :, :]\n",
    "\n",
    "        # Rotate images randomly\n",
    "        rotation_rand = np.random.randint(0, 3)\n",
    "        if(rotation_rand==1):\n",
    "            augmented_data[i] = np.rot90(augmented_data[i], axes=(0, 1))\n",
    "        if(rotation_rand==2):\n",
    "            augmented_data[i] = np.rot90(augmented_data[i], axes=(1, 0))\n",
    "\n",
    "        # Draw mask for spots addition\n",
    "        mask = infer_mask(augmented_data[i, :, :])\n",
    "\n",
    "        # Add random number of damage sites\n",
    "        damage_site_num = np.random.randint(damaged_site_range[0], damaged_site_range[1])\n",
    "\n",
    "        GT_counts[i] = damage_site_num\n",
    "        spots_locations = []\n",
    "\n",
    "        # Add PSF shape in random location inside the cell's mask\n",
    "        for j in range(damage_site_num):\n",
    "            mask_arr = np.array(np.where(mask == 1))\n",
    "            rng = int((PSF_size-1)/2)\n",
    "            sigma = np.random.uniform(0.75, 2.25)\n",
    "\n",
    "            # Determine the PSF location (it has to be inside the cell and far enough from other PSFs)\n",
    "            while(True):\n",
    "                PSF_loc_ind = np.random.choice(np.arange(mask_arr.shape[1]))\n",
    "                xy = mask_arr[:, PSF_loc_ind]+np.random.normal(0, 1, 2)\n",
    "                if(xy[0]+rng+1 < row and xy[0] - rng > 0 and xy[1]+rng+1 < col and xy[1] - rng > 0):\n",
    "                    good_loc_flag = True\n",
    "                    for k in range(len(spots_locations)):\n",
    "                        if(np.abs(xy[0]-spots_locations[k][0]) < rng and np.abs(xy[1]-spots_locations[k][1]) < rng):\n",
    "                            good_loc_flag = False\n",
    "                            break\n",
    "                    if(good_loc_flag==True):\n",
    "                        spots_locations.append(xy)\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            amp = 3 * (sigma ** 1.5) * np.random.uniform(np.percentile(data[int(i / size_increase), int(xy[0]-rng):int(xy[0]+rng+1), int(xy[1]-rng):int(xy[1]+rng+1)], 75),\n",
    "                                                np.max(data[int(i / size_increase), int(xy[0]-rng):int(xy[0]+rng+1), int(xy[1]-rng):int(xy[1]+rng+1)]))\n",
    "            PSF_to_draw = draw_PSF(PSF_size, amp, sigma, xy)\n",
    "            PSF_GT = draw_PSF(PSF_size, 1, sigma, xy)\n",
    "\n",
    "            augmented_data[i, int(xy[0]-rng):int(xy[0]+rng+1), int(xy[1]-rng):int(xy[1]+rng+1)] += PSF_to_draw\n",
    "            ground_truth[i, int(xy[0]-rng):int(xy[0]+rng+1), int(xy[1]-rng):int(xy[1]+rng+1)] += PSF_GT\n",
    "\n",
    "        # Project values\n",
    "        augmented_data[i, :, :] = project_percentile(augmented_data[i, :, :], 90)\n",
    "        ground_truth[i, :, :] = (row * col) * project_01(ground_truth[i, :, :])\n",
    "\n",
    "    return augmented_data, ground_truth, GT_counts\n",
    "\n",
    "def CreateDataLoader(X, y, batch_size):\n",
    "    \"\"\"\n",
    "        Creates data loader for X, y pairs\n",
    "        :param X: Tensor [# of observations, numOfBins] for the observations\n",
    "        :param y: Tensor [# of observations] for the predictions\n",
    "        :param batch_size: Int for batch size\n",
    "        :return: data loader for training/ testing routines\n",
    "    \"\"\"\n",
    "    data_loader = torch.utils.data.DataLoader(TensorDataset(X, y), batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "def gauss2d_assymetric(xy, offset, amp, x0, y0, sigma_x, sigma_y):\n",
    "    # Fit patch to gaussian\n",
    "    x, y = xy\n",
    "    return offset + amp * np.exp(-(((x - x0) ** 2)/ (2 * sigma_x ** 2) + ((y - y0) ** 2) / (2 * sigma_y ** 2)))\n",
    "\n",
    "def save_data(X, y):\n",
    "    np.save('X', X)\n",
    "    np.save('y', y)\n",
    "\n",
    "def load_data():\n",
    "    X = np.load('X.npy')\n",
    "    y = np.load('y.npy')\n",
    "    return [X, y]\n",
    "\n",
    "def LoadTIFF(file, path):\n",
    "    tiff = io.imread(os.path.join(path, file))\n",
    "    data_shape = tiff[0].shape\n",
    "    length = tiff.shape[0]\n",
    "\n",
    "    data = np.zeros((length, data_shape[0], data_shape[1]))\n",
    "    for i in range(length):\n",
    "        data[i, :data_shape[0], :data_shape[1]] = tiff[i]\n",
    "\n",
    "    return data\n",
    "\n",
    "def ExportTIFF(X_test, y_test_pred, augmented_data_test, data_path):\n",
    "    for i in range(X_test.shape[0]):\n",
    "        fig = plt.figure()\n",
    "        [spots_count, spots_coord, confidence, _] = count_molecules(X_test.data.numpy()[i, 0],\n",
    "                                                                 y_test_pred.data.numpy()[i, 0], patch_length=5,\n",
    "                                                                 conf_threshold=100, verbose=False)\n",
    "        plt.imshow(augmented_data_test[i])\n",
    "        if(spots_coord.shape[0] != 0):\n",
    "            plt.scatter(spots_coord[:, 1], spots_coord[:, 0], marker='x', c='r')\n",
    "        fig.savefig(os.path.join(data_path, 'input_{}.tif'.format(i)))\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        fig2 = plt.figure()\n",
    "        [spots_count, spots_coord, confidence, _] = count_molecules(X_test.data.numpy()[i, 0],\n",
    "                                                                 y_test_pred.data.numpy()[i, 0], patch_length=5,\n",
    "                                                                 conf_threshold=100, verbose=False)\n",
    "        plt.imshow(y_test_pred.data[i, 0])\n",
    "        if (spots_coord.shape[0] != 0):\n",
    "            coord = np.concatenate([np.zeros([1, 2]), spots_coord, np.zeros([1, 2])])\n",
    "            plt.scatter(coord[:, 1], coord[:, 0], marker='x', c='r')\n",
    "        fig2.savefig(os.path.join(data_path, 'net_output_{}.tif'.format(i)))\n",
    "\n",
    "def ExportTIFF_clean(X_test, y_test_pred, augmented_data_test, data_path):\n",
    "    for i in range(X_test.shape[0]):\n",
    "        fig = plt.figure()\n",
    "        [spots_count, spots_coord, confidence, _] = count_molecules(X_test.data.numpy()[i, 0],\n",
    "                                                                 y_test_pred.data.numpy()[i, 0], patch_length=5,\n",
    "                                                                 conf_threshold=100, verbose=False)\n",
    "        plt.imshow(augmented_data_test[i])\n",
    "        fig.savefig(os.path.join(data_path, 'input_{}.tif'.format(i)))\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        fig2 = plt.figure()\n",
    "        [spots_count, spots_coord, confidence, _] = count_molecules(X_test.data.numpy()[i, 0],\n",
    "                                                                 y_test_pred.data.numpy()[i, 0], patch_length=5,\n",
    "                                                                 conf_threshold=100, verbose=False)\n",
    "        plt.imshow(y_test_pred.data[i, 0])\n",
    "        fig2.savefig(os.path.join(data_path, 'net_output_{}.tif'.format(i)))\n",
    "\n",
    "def ShrinkImages(data, size, save_coord=False):\n",
    "    [H, W] = data.shape\n",
    "    from scipy import ndimage\n",
    "    new_data = 30 * np.ones([size, size])\n",
    "    data_filt = np.zeros_like(data)\n",
    "    data_filt[np.where(data > np.median(data))] = data[np.where(data > np.median(data))]\n",
    "    [x0, y0] = ndimage.measurements.center_of_mass(data_filt)\n",
    "    max_ind = np.argmax(data)\n",
    "    x0 = (x0 + max_ind % W) / 2\n",
    "    y0 = (y0 + max_ind / W) / 2\n",
    "    left = np.max([0, int(x0 - size/2)])\n",
    "    right = np.min([int(x0 + size/2), W])\n",
    "    down = np.max([0, int(y0 - size/2)])\n",
    "    up = np.min([int(y0 + size/2), H])\n",
    "    new_data[:up-down, :right-left] = data[down:up, left:right]\n",
    "    if save_coord:\n",
    "        return new_data, np.array([down, left])\n",
    "    else:\n",
    "        return new_data\n",
    "\n",
    "\n",
    "import scipy.optimize as opt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.signal import butter,filtfilt\n",
    "\n",
    "def gauss2d(xy, offset, amp, x0, y0, sigma):\n",
    "    # Fit patch to gaussian\n",
    "    x, y = xy\n",
    "    return offset + amp * np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "def count_molecules(orig_img, img, patch_length, conf_threshold, verbose):\n",
    "    mask = np.zeros_like(img)\n",
    "    percentile = np.percentile(orig_img, 85)\n",
    "    indices = np.where(orig_img > percentile)\n",
    "    mask[indices] = 1\n",
    "\n",
    "    emitters_cnt = 0\n",
    "    emitters_coordinates = []\n",
    "    intensity = []\n",
    "    confidence = []\n",
    "    max_row, max_col = img.shape\n",
    "    xy = np.zeros([2, int(patch_length ** 2)])\n",
    "    for i1 in range(patch_length):\n",
    "        for j1 in range(patch_length):\n",
    "            xy[:, int(i1 + patch_length * j1)] = [i1, j1]\n",
    "\n",
    "    for i in range(len(indices[0])):\n",
    "        row, col = indices[0][i], indices[1][i]\n",
    "\n",
    "        if(img[int(row), int(col)] < 0):\n",
    "            continue\n",
    "\n",
    "        # Handle clusters in case of exceeding image shape\n",
    "        up = int(row + np.floor(patch_length / 2)) + 1\n",
    "        down = int(row - np.floor(patch_length / 2))\n",
    "        left = int(col - np.floor(patch_length / 2))\n",
    "        right = int(col + np.floor(patch_length / 2)) + 1\n",
    "\n",
    "        # Ignore out of bound blinks\n",
    "        if up > max_row or down < 0 or left < 0 or right > max_col:\n",
    "            continue\n",
    "\n",
    "        # Check if localization is in local maximum\n",
    "        local_max = np.max(img[int(row - 1):int(row + 2), int(col - 1):int(col + 2)])\n",
    "        if (local_max * 0.8 > img[int(row), int(col)]):\n",
    "            if verbose == True:\n",
    "                print(\"Not local max:\", local_max, img[int(row), int(col)])\n",
    "            continue\n",
    "\n",
    "        # Initial guess\n",
    "        x0, y0 = int(np.floor(patch_length / 2)), int(np.floor(patch_length / 2))\n",
    "\n",
    "        # Fit the patch to a gaussian\n",
    "        zobs = (img[down:up, left:right]).reshape(1, -1).squeeze()\n",
    "\n",
    "        guess = [np.median(img), np.max(img) - np.min(img), x0, y0, 1]\n",
    "        try:\n",
    "            pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        fit = gauss2d(xy, *pred_params)\n",
    "\n",
    "        # Handle same coordinate repetition\n",
    "        y = down + pred_params[3]\n",
    "        x = left + pred_params[2]\n",
    "\n",
    "        if(pred_params[4] < 0.45 or pred_params[4] > 4 or pred_params[1] < 0\n",
    "            or pred_params[1] < 0.15 * np.max(img)\n",
    "            or pred_params[2] < 0 or pred_params[2] > patch_length\n",
    "            or pred_params[3] < 0 or pred_params[3] > patch_length):\n",
    "            if(verbose == True):\n",
    "                print(\"row\", y, \"col\", x, \"bad fitting params\", pred_params)\n",
    "            continue\n",
    "\n",
    "        # If the peak is higher than threshold proceed\n",
    "        curr_row = int(np.round(down + pred_params[3]))\n",
    "        curr_col = int(np.round(left + pred_params[2]))\n",
    "        if (curr_col >= max_col or curr_col < 0 or curr_row >= max_row or curr_row < 0):\n",
    "            continue\n",
    "\n",
    "        too_close_flag = False\n",
    "        for k in range(len(emitters_coordinates)):\n",
    "            if (np.abs(y - emitters_coordinates[k][0]) < 1.5 and np.abs(x - emitters_coordinates[k][1]) < 1.5):\n",
    "                too_close_flag = True\n",
    "                break\n",
    "        if (too_close_flag == True):\n",
    "            continue\n",
    "\n",
    "        # Calculate RMS\n",
    "        zobs /= np.max(zobs)\n",
    "        fit /= np.max(fit)\n",
    "        fit_quality = 1 - np.sqrt(np.mean((zobs - fit) ** 2))\n",
    "\n",
    "        # If the fit quality is higher than Value > take the mean value as a new cluster's coordinates\n",
    "        # Ignore fitted gaussian with sigma higher than 1 or lower than 0.3\n",
    "        if (fit_quality > 0.85):\n",
    "            confidence_lvl = fit_quality * np.max(img[int(row - int(patch_length/2)):int(row + int(patch_length/2)+1),\n",
    "                                                  int(col - int(patch_length/2)):int(col + int(patch_length/2)+1)])\n",
    "            confidence_lvl = np.min([2000, confidence_lvl])\n",
    "            confidence_lvl /= 2\n",
    "            if(confidence_lvl < conf_threshold):\n",
    "                if (verbose == True):\n",
    "                    print(\"row\", y, \"col\", x, \"confidence level too low:\", confidence_lvl)\n",
    "                continue\n",
    "            # Add localization\n",
    "            emitters_cnt += 1\n",
    "            # Update emitters list\n",
    "            emitters_coordinates.append([y, x])\n",
    "            # Update confidence vector\n",
    "            confidence.append(confidence_lvl)\n",
    "\n",
    "            smooth_img = gaussian_filter(orig_img, sigma=7)\n",
    "            LPF_img = butter_lowpass_filter(smooth_img, 0.5, 20)\n",
    "            filtered_img = orig_img - LPF_img\n",
    "\n",
    "            # Add localization intensity\n",
    "            intensity.append(np.sum(filtered_img[(int(y) - 1):(int(y) + 2), (int(x) - 1):(int(x) + 2)]))\n",
    "\n",
    "            if (verbose == True):\n",
    "                print(\"row\", y, \"col\", x, \"=========== is emitter!!! ===========\", pred_params, \"fit quality\", fit_quality, \"confidence\", confidence_lvl)\n",
    "        else:\n",
    "            if(verbose == True):\n",
    "                print(\"row\", y, \"col\", x, \"bad fit quality\", fit_quality)\n",
    "    return emitters_cnt, np.array(emitters_coordinates), np.array(confidence), np.array(intensity)\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, order):\n",
    "    # Get the filter coefficients\n",
    "    b, a = butter(order, cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def project_01(im):\n",
    "    if(im.sum() == 0):\n",
    "        return im\n",
    "    # Squeeze the pixels value to be in range [0, 1]\n",
    "    im = np.squeeze(im)\n",
    "    min_val = im.min()\n",
    "    max_val = im.max()\n",
    "    return (im - min_val) / (max_val - min_val)\n",
    "\n",
    "def project_percentile(im, percentile):\n",
    "    if (im.sum() == 0):\n",
    "        return im\n",
    "        # Squeeze the pixels value to be in range [0, 1]\n",
    "    im = np.squeeze(im)\n",
    "    percentile_val = np.percentile(im, percentile)\n",
    "    comp_percentile_val = np.percentile(im, 2*(100 - percentile))\n",
    "    return (im - comp_percentile_val) / percentile_val\n",
    "\n",
    "def luc_debug(data, X_test, y_test_pred, i):\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(data[i])\n",
    "    plt.title('raw image')\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(X_test[i])\n",
    "    plt.title('cropped image')\n",
    "    plt.subplot(223)\n",
    "    [spots_count, spots_coord, confidence, _] = count_molecules(X_test.data.numpy()[i, 0], y_test_pred.data.numpy()[0, 0], patch_length=5, conf_threshold=85, verbose=True)\n",
    "    plt.imshow(y_test_pred.data[i, 0])\n",
    "    if(spots_coord.shape[0] != 0):\n",
    "        coord = np.concatenate([np.zeros([1, 2]), spots_coord, np.zeros([1, 2])])\n",
    "        confidence = np.concatenate([np.zeros(1), confidence, 100 * np.ones(1)])\n",
    "        plt.scatter(coord[:, 1], coord[:, 0], marker='x', c=confidence, cmap='seismic')\n",
    "    plt.title('prediction {} spots'.format(spots_count))\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(X_test[i])\n",
    "    if(spots_coord.shape[0] != 0):\n",
    "        plt.scatter(spots_coord[:, 1], spots_coord[:, 0], marker='x', c=confidence, cmap='seismic')\n",
    "    plt.title('detected spots over original image')\n",
    "    plt.show()\n",
    "\n",
    "def get_counts_intensity_per_exp(data_path, filename):\n",
    "    target = os.path.join(data_path, filename)\n",
    "    if os.path.getsize(target) > 0:\n",
    "        with open(target, \"rb\") as a_file:\n",
    "            unpickler = pickle.Unpickler(a_file)\n",
    "            cell_dict = unpickler.load()\n",
    "            a_file.close()\n",
    "\n",
    "            sum_cnt = 0\n",
    "            sum_intensity = 0\n",
    "            counter = 0\n",
    "            for key in cell_dict:\n",
    "                sum_cnt += cell_dict[key][3]\n",
    "                sum_intensity += np.sum(cell_dict[key][5])\n",
    "                counter += 1\n",
    "        print(\"==== {} ====\".format(filename))\n",
    "        print(\"Average foci num:\", sum_cnt/counter)\n",
    "        print(\"Total intensity:\", sum_intensity)\n",
    "        return sum_cnt/counter, sum_intensity\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL.Image\n",
    "\n",
    "class Trainer():\n",
    "    \"\"\"\n",
    "    A class abstracting the various tasks of training models.\n",
    "    Provides methods at multiple levels of granularity:\n",
    "    - Multiple epochs (fit)\n",
    "    - Single epoch (train_epoch/test_epoch)\n",
    "    - Single batch (train_batch/test_batch)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optimizer, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the trainer.\n",
    "        :param model: Instance of the model to train.\n",
    "        :param loss_fn: The loss function to evaluate with.\n",
    "        :param optimizer: The optimizer to train with.\n",
    "        :param device: torch.device to run training on (CPU or GPU).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self, dl_train: DataLoader, dl_test: DataLoader,\n",
    "            num_epochs, early_stopping=50, print_every=1, **kw):\n",
    "        \"\"\"\n",
    "        Trains the model for multiple epochs with a given training set,\n",
    "        and calculates validation loss over a given validation set.\n",
    "        :param dl_train: Dataloader for the training set.\n",
    "        :param dl_test: Dataloader for the test set.\n",
    "        :param num_epochs: Number of epochs to train for.\n",
    "        :param early_stopping: Whether to stop training early if there is no\n",
    "            test loss improvement for this number of epochs.\n",
    "        :param print_every: Print progress every this number of epochs.\n",
    "        :return: Train and test losses per epoch.\n",
    "        \"\"\"\n",
    "        train_loss, val_loss = [], []\n",
    "        best_loss = None\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'--- EPOCH {epoch + 1}/{num_epochs} ---')\n",
    "\n",
    "            loss = self.train_epoch(dl_train, **kw)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            loss = self.test_epoch(dl_test, epoch, **kw)\n",
    "            val_loss.append(loss)\n",
    "\n",
    "            if(epoch == 0):\n",
    "                best_loss = loss\n",
    "            else:\n",
    "                if(loss + 1e-5 >= best_loss):\n",
    "                    epochs_without_improvement += 1\n",
    "                    if(epochs_without_improvement > early_stopping):\n",
    "                        print(\"Reached early stopping criterion\")\n",
    "                        self.model.load_state_dict(torch.load('best_model'))\n",
    "                        break\n",
    "                else:\n",
    "                    epochs_without_improvement = 0\n",
    "                    best_loss = loss\n",
    "                    torch.save(self.model.state_dict(), 'best_model')\n",
    "\n",
    "            if epoch % print_every == 0 or epoch == num_epochs - 1:\n",
    "                print(\"Train loss =\", train_loss[-1])\n",
    "                print(\"Validation loss =\", val_loss[-1])\n",
    "\n",
    "\n",
    "    def train_epoch(self, dl_train: DataLoader, **kw):\n",
    "        \"\"\"\n",
    "        Train once over a training set (single epoch).\n",
    "        :param dl_train: DataLoader for the training set.\n",
    "        :param kw: Keyword args supported by _foreach_batch.\n",
    "        :return: An EpochResult for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss_num = 0\n",
    "        cnt = 0\n",
    "        lam = 5\n",
    "\n",
    "        for X_train, y_train in dl_train:\n",
    "            X_train = X_train.to(self.device)\n",
    "            y_train = y_train.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = self.model(X_train)\n",
    "\n",
    "            # Compute Loss\n",
    "            reg = len(torch.where(y_pred > 10))\n",
    "            loss = self.loss_fn(y_pred, y_train.squeeze()) + lam * reg\n",
    "\n",
    "            total_loss = loss\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss_num += total_loss.item()\n",
    "\n",
    "            cnt += 1\n",
    "        return total_loss_num / cnt\n",
    "\n",
    "    def test_epoch(self, dl_test: DataLoader, epoch_num, **kw):\n",
    "        \"\"\"\n",
    "        Evaluate model once over a test set (single epoch).\n",
    "        :param dl_test: DataLoader for the test set.\n",
    "        :param kw: Keyword args supported by _foreach_batch.\n",
    "        :return: An EpochResult for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for X_test, y_test in dl_test:\n",
    "            X_test = X_test.to(self.device)\n",
    "            y_test = y_test.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = self.model(X_test)\n",
    "            loss = self.loss_fn(y_pred, y_test.squeeze()).item()\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "\n",
    "            '''if (cnt == 1 or cnt == 5 or cnt == 15):\n",
    "                im = PIL.Image.fromarray(y_pred[0, 0].data.numpy())\n",
    "                im.save('output/val_pred_img_{}_{}.tif'.format(epoch_num, cnt))\n",
    "                if (epoch_num == 0):\n",
    "                    im = PIL.Image.fromarray(X_test[0, 0].data.numpy())\n",
    "                    im.save('output/val_obs_img_{}_{}.tif'.format(epoch_num, cnt))\n",
    "                    im = PIL.Image.fromarray(y_test[0, 0].data.numpy())\n",
    "                    im.save('output/val_test_img_{}_{}.tif'.format(epoch_num, cnt))'''\n",
    "\n",
    "\n",
    "        return total_loss / cnt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_test, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.PReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.PReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=256, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.PReLU())\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=64, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.PReLU())\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=32, kernel_size=5, padding=2, padding_mode='replicate'),\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.PReLU())\n",
    "        self.layer6 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=1, kernel_size=5, padding=2, padding_mode='replicate'))\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "\n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(torch.cat([x4, x2], dim=1))\n",
    "        x6 = self.layer6(torch.cat([x5, x1], dim=1))\n",
    "\n",
    "        return x6\n",
    "1\n",
    "!pip install tiffcapture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4f1f1",
   "metadata": {},
   "source": [
    "### Device Selection\n",
    "In the next cell you may specify on which device you wish to run the code. Default device: cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d51d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "Device = 'cuda:0' #@param {type:\"string\"}\n",
    "# Device\n",
    "device = torch.device(Device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28ba35",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Network\n",
    "Please specify the directory of the pre-trained neural network you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae30d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_path = 'C:\\\\Users\\\\WeissLab\\\\Data\\\\Flow-based imaging\\\\Code'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae7e37",
   "metadata": {},
   "source": [
    "Run this cell to load the pre-trained neural network. Make sure to change the name of the net to correspond to the one you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a4920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CNN_test().to(device)\n",
    "net.load_state_dict(torch.load(os.path.join(DNN_path, 'CNN_model'), map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9dc9a",
   "metadata": {},
   "source": [
    "### Running the Analysis\n",
    "Please specify the path to the input tiffs and the path to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c73395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'C:\\\\Users\\\\WeissLab\\\\Data\\\\Flow-based imaging\\\\ImageData\\\\20220330'#@param {type:\"string\"}\n",
    "output_path = 'C:\\\\Users\\\\WeissLab\\\\Data\\\\Flow-based imaging\\\\ImageData\\\\20220330_JNtests'#@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496755d",
   "metadata": {},
   "source": [
    "#### **Parameters**:\n",
    "\n",
    "conf_threshold - specify the minimal confidence level a potential damage site should have to be valid. Valid values range = [0, 100]\n",
    "\n",
    "shrink_size - specify the cropped image size (both height and width). Valid values range < min{input image height, input image width}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc38a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_threshold = 100\n",
    "shrink_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39309da",
   "metadata": {},
   "source": [
    "Run this cell to analyze every tiff in the input_path directory. The results will be save at the output_path directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a14bfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Analyzing 06hr_RAG2 i53F_4_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "================= Analyzing 06hr_RAG2 i53F_4_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "================= Analyzing 06hr_RAG2 i53_3_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "An exception occured with the localization map\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "================= Analyzing 06hr_RAG2 i53_3_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "================= Analyzing 06hr_RAG2 only_2_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "================= Analyzing 06hr_RAG2 only_2_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "================= Analyzing 06hr_Untreated_1_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "Working on image number: 2200\n",
      "Working on image number: 2300\n",
      "Working on image number: 2400\n",
      "Working on image number: 2500\n",
      "================= Analyzing 06hr_Untreated_1_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "Working on image number: 2200\n",
      "Working on image number: 2300\n",
      "Working on image number: 2400\n",
      "Working on image number: 2500\n",
      "================= Analyzing 12hr_RAG2 i53F_8_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "Working on image number: 2200\n",
      "Working on image number: 2300\n",
      "Working on image number: 2400\n",
      "Working on image number: 2500\n",
      "================= Analyzing 12hr_RAG2 i53F_8_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "Working on image number: 2200\n",
      "Working on image number: 2300\n",
      "Working on image number: 2400\n",
      "Working on image number: 2500\n",
      "================= Analyzing 12hr_RAG2 i53_7_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "================= Analyzing 12hr_RAG2 i53_7_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "================= Analyzing 12hr_RAG2 only_6_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "================= Analyzing 12hr_RAG2 only_6_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "================= Analyzing 12hr_untreated_5_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "================= Analyzing 12hr_untreated_5_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "================= Analyzing 18hr_RAG2 i53F_12_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "================= Analyzing 18hr_RAG2 i53F_12_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "================= Analyzing 18hr_RAG2 i53_11_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "================= Analyzing 18hr_RAG2 i53_11_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "================= Analyzing 18hr_RAG2 only_10_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "================= Analyzing 18hr_RAG2 only_10_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "================= Analyzing 18hr_untreated_9_all_ch2.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "Working on image number: 2200\n",
      "================= Analyzing 18hr_untreated_9_all_ch4.tif =================\n",
      "Working on image number: 0\n",
      "Working on image number: 100\n",
      "Working on image number: 200\n",
      "Working on image number: 300\n",
      "Working on image number: 400\n",
      "Working on image number: 500\n",
      "Working on image number: 600\n",
      "Working on image number: 700\n",
      "Working on image number: 800\n",
      "Working on image number: 900\n",
      "Working on image number: 1000\n",
      "Working on image number: 1100\n",
      "Working on image number: 1200\n",
      "Working on image number: 1300\n",
      "Working on image number: 1400\n",
      "Working on image number: 1500\n",
      "Working on image number: 1600\n",
      "Working on image number: 1700\n",
      "Working on image number: 1800\n",
      "Working on image number: 1900\n",
      "Working on image number: 2000\n",
      "Working on image number: 2100\n",
      "Working on image number: 2200\n",
      "Data analysis finished successfully\n"
     ]
    }
   ],
   "source": [
    "tif_save_path = os.path.join(output_path, 'tmp')\n",
    "if(not os.path.isdir(tif_save_path)):\n",
    "    os.mkdir(tif_save_path)\n",
    "\n",
    "tif_out_path = os.path.join(output_path, 'out_tiffs')\n",
    "if(not os.path.isdir(tif_out_path)):\n",
    "    os.mkdir(tif_out_path)\n",
    "\n",
    "# Remove temporary tiff files\n",
    "for file in os.listdir(tif_save_path):\n",
    "    if(not os.path.isdir(os.path.join(tif_save_path, file))):\n",
    "        os.remove(os.path.join(tif_save_path, file))\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".tif\"):\n",
    "        print(\"================= Analyzing\", filename, \"=================\")\n",
    "        cell_dict = {}\n",
    "        data = LoadTIFF(filename, input_path)\n",
    "        L, M, N = data.shape\n",
    "        augmented_data_test = np.zeros([L, shrink_size, shrink_size])\n",
    "        X_test = np.zeros([L, shrink_size, shrink_size])\n",
    "        shift_coord = np.zeros([L, 2])\n",
    "        for i in range(L):\n",
    "            shrinked_img, shift_coord[i] = ShrinkImages(data[i], shrink_size, save_coord=True)\n",
    "            augmented_data_test[i, :, :] = project_percentile(shrinked_img, 90)\n",
    "            X_test[i, :, :] = augmented_data_test[i, :, :]\n",
    "        [N3, H, W] = X_test.shape\n",
    "        X_test = torch.FloatTensor(X_test.reshape([N3, 1, H, W]))\n",
    "\n",
    "        cnt_pred = np.zeros(X_test.shape[0])\n",
    "        X_test = X_test.to(device)\n",
    "        for i in range(X_test.shape[0]):\n",
    "            if (i % 100 == 0):\n",
    "                print(\"Working on image number:\", i)\n",
    "            y_test_pred = net(X_test[i:i + 1])\n",
    "            # Create temporary riff files for each net output\n",
    "            padded_img = np.zeros([M, N])\n",
    "            if(shift_coord[i, 0]+shrink_size < M and shift_coord[i, 1]+shrink_size < N):\n",
    "                padded_img[int(shift_coord[i, 0]):int(shift_coord[i, 0]+shrink_size),\n",
    "                          int(shift_coord[i, 1]):int(shift_coord[i, 1]+shrink_size)] = y_test_pred.detach().cpu().numpy()[0,0]\n",
    "            else:\n",
    "                up = int(np.min([shift_coord[i, 0]+shrink_size, M]))\n",
    "                right = int(np.min([shift_coord[i, 1]+shrink_size, N]))\n",
    "                padded_img[int(shift_coord[i, 0]):up, int(shift_coord[i, 1]):right] = y_test_pred.detach().cpu().numpy()[0,0,:int(M-shift_coord[i, 0]),:int(N-shift_coord[i, 1])]\n",
    "            \n",
    "            io.imsave(os.path.join(tif_save_path, 'img_{}.tif'.format(i)), padded_img)\n",
    "            shrinked_img, _ = ShrinkImages(data[i], shrink_size, save_coord=True)\n",
    "            [cnt_pred[i], coord, conf_lvl, intensity] = count_molecules(shrinked_img,\n",
    "                                                  y_test_pred.detach().cpu().numpy()[0, 0],\n",
    "                                                  patch_length=5, conf_threshold=conf_threshold, verbose=False)\n",
    "            cell_dict[i] = [shift_coord[i], coord, intensity, cnt_pred[i], conf_lvl, np.sum(intensity)]\n",
    "        \n",
    "            # Create localization maps\n",
    "            loc_map = np.zeros([M, N])\n",
    "            for j in range(coord.shape[0]):\n",
    "                try:\n",
    "                    loc_map[int(shift_coord[i, 0] + coord[j, 0]), int(shift_coord[i, 1] + coord[j, 1])] += 255\n",
    "                except:\n",
    "                    print(\"An exception occured with the localization map\")\n",
    "            io.imsave(os.path.join(tif_save_path, 'loc_{}.tif'.format(i)), loc_map)\n",
    "\n",
    "            # Save pickle files\n",
    "            a_file = open(os.path.join(output_path, \"{}.pkl\".format(filename)), \"wb\")\n",
    "            pickle.dump(cell_dict, a_file)\n",
    "            a_file.close()\n",
    "\n",
    "        # Combine tiff files to a stack\n",
    "        s1 = tifffile.TiffWriter(os.path.join(tif_out_path, '{}_net_outputs.tif'.format(filename[:-4])))\n",
    "        s2 = tifffile.TiffWriter(os.path.join(tif_out_path, '{}_localization_map.tif'.format(filename[:-4])))\n",
    "        import re\n",
    "        for filename in sorted(glob.glob(os.path.join(tif_save_path, '*.tif')), key=lambda name: int(re.split(r\"[_.]\",name)[-2])):\n",
    "            \n",
    "            if 'img_' in filename:\n",
    "                s1.save(tifffile.imread(filename))\n",
    "            elif 'loc_' in filename:\n",
    "                s2.save(tifffile.imread(filename))\n",
    "\n",
    "print(\"Data analysis finished successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3461ca",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06eed21",
   "metadata": {},
   "source": [
    "### IS THIS PART OF THE CODE NECESSARY ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08849c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "A_cnt_list = []\n",
    "A_int_list = []\n",
    "A_filenames = []\n",
    "B_cnt_list = []\n",
    "B_int_list = []\n",
    "B_filenames = []\n",
    "C_cnt_list = []\n",
    "C_int_list = []\n",
    "C_filenames = []\n",
    "D_cnt_list = []\n",
    "D_int_list = []\n",
    "D_filenames = []\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        avg_cnt, sum_int = get_counts_intensity_per_exp(input_path, filename)\n",
    "        if filename.startswith('A'):\n",
    "            A_cnt_list.append(avg_cnt)\n",
    "            A_int_list.append(sum_int)\n",
    "            A_filenames.append(filename)\n",
    "        if filename.startswith('B'):\n",
    "            B_cnt_list.append(avg_cnt)\n",
    "            B_int_list.append(sum_int)\n",
    "            B_filenames.append(filename)\n",
    "        if filename.startswith('C'):\n",
    "            C_cnt_list.append(avg_cnt)\n",
    "            C_int_list.append(sum_int)\n",
    "            C_filenames.append(filename)\n",
    "        if filename.startswith('D'):\n",
    "            D_cnt_list.append(avg_cnt)\n",
    "            D_int_list.append(sum_int)\n",
    "            D_filenames.append(filename)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.scatter(np.arange(len(A_int_list)), A_int_list)\n",
    "plt.xticks(np.arange(len(A_int_list)), A_filenames)\n",
    "plt.title('Total foci intensity')\n",
    "plt.subplot(212)\n",
    "plt.scatter(np.arange(len(A_cnt_list)), A_cnt_list)\n",
    "plt.xticks(np.arange(len(A_cnt_list)), A_filenames)\n",
    "plt.ylim([0, 5])\n",
    "plt.title('Average number of foci')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.scatter(np.arange(len(B_int_list)), B_int_list)\n",
    "plt.xticks(np.arange(len(B_int_list)), B_filenames)\n",
    "plt.title('Total foci intensity')\n",
    "plt.subplot(212)\n",
    "plt.scatter(np.arange(len(B_cnt_list)), B_cnt_list)\n",
    "plt.xticks(np.arange(len(B_cnt_list)), B_filenames)\n",
    "plt.ylim([0, 5])\n",
    "plt.title('Average number of foci')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.scatter(np.arange(len(C_int_list)), C_int_list)\n",
    "plt.xticks(np.arange(len(C_int_list)), C_filenames)\n",
    "plt.title('Total foci intensity')\n",
    "plt.subplot(212)\n",
    "plt.scatter(np.arange(len(C_cnt_list)), C_cnt_list)\n",
    "plt.xticks(np.arange(len(C_cnt_list)), C_filenames)\n",
    "plt.ylim([0, 5])\n",
    "plt.title('Average number of foci')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.scatter(np.arange(len(D_int_list)), D_int_list)\n",
    "plt.xticks(np.arange(len(D_int_list)), D_filenames)\n",
    "plt.title('Total foci intensity')\n",
    "plt.subplot(212)\n",
    "plt.scatter(np.arange(len(D_cnt_list)), D_cnt_list)\n",
    "plt.xticks(np.arange(len(D_cnt_list)), D_filenames)\n",
    "plt.ylim([0, 5])\n",
    "plt.title('Average number of foci')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464964a6",
   "metadata": {},
   "source": [
    "### Export Results as TIFFs\n",
    "Please specify the file name and the range of indices you wish to export:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bab458",
   "metadata": {},
   "source": [
    "### CONFUSED ABOUT WHAT THAT DOES AND WHAT PATH TO PUT BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22132e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/content/gdrive/MyDrive/CRISPR data/CC_20k_AAV_only_2_all_ch2.tif' #@param {type:\"string\"}\n",
    "start_index = 10 #@param {type:\"number\"}\n",
    "end_index =  20#@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eac90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LoadTIFF(filename, input_path)\n",
    "L = end_index - start_index\n",
    "if(start_index > data.shape[0] or end_index-start_index > data.shape[0]):\n",
    "  print(\"TIFF Stack is smaller than the specified indices. Please specify valid index range\")\n",
    "else:\n",
    "  augmented_data_test = np.zeros([L, shrink_size, shrink_size])\n",
    "  X_test = np.zeros([L, shrink_size, shrink_size])\n",
    "  for i in range(end_index-start_index):\n",
    "      shrinked_img, shift_coord = ShrinkImages(data[i + start_index], shrink_size, save_coord=True)\n",
    "      augmented_data_test[i, :, :] = project_percentile(shrinked_img, 90)\n",
    "      X_test[i, :, :] = augmented_data_test[i, :, :]\n",
    "  [N3, H, W] = X_test.shape\n",
    "  X_test = torch.FloatTensor(X_test.reshape([N3, 1, H, W]))\n",
    "  X_test = X_test.to(device)\n",
    "\n",
    "  net = CNN_test().to(device)\n",
    "  net.load_state_dict(torch.load(os.path.join(DNN_path, 'CNN_model')), map_location=torch.device(device))\n",
    "  y_test_pred = net(X_test)\n",
    "  ExportTIFF(X_test.cpu(), y_test_pred.cpu(), augmented_data_test[:end_index-start_index], output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
